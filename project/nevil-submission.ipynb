{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Full Stack Machine Learning's Week 3 Project!\n",
    "\n",
    "Welcome to the taxi fare prediction project! In this project, you will be taking on the role of a Data Scientist tasked with developing a model that can predict the fare for a taxi ride in New York City. The [dataset](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page) we will be using contains records of trips made by yellow and green taxis, as well as records of For-Hire Vehicle (FHV) trips.\n",
    "\n",
    "As a Data Scientist, your goal will be to use this data to build an accurate model that can predict taxi fares. You will learn how to preprocess the data and train a model. Additionally, you will be introduced to event-driven Machine Learning pipelines and learn how to trigger these pipelines in the cloud using Metaflow.\n",
    "\n",
    "By the end of this project, you will have gained valuable experience in building and evaluating Machine Learning models for real-world applications, as well as a deeper understanding of how to run pipelines in the cloud."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0: Install new Metaflow version to use event-triggering features\n",
    "\n",
    "Run the next cell to update to the latest Metaflow release with event-triggering support. Note you only need to run it one time in the `full-stack-metaflow-corise` environment in your sandbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install -U metaflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: EDA\n",
    "\n",
    "You can find the [original `.parquet` file here](https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet).\n",
    "\n",
    "Suggestion: This week is primarily about task 2-3, and 4 if you have time. Don't spend more than an hour on Task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from metaflow import S3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# configuration\n",
    "YELLOW = '#FFBC00'\n",
    "GREEN = '#37795D'\n",
    "PURPLE = '#5460C0'\n",
    "BACKGROUND = '#F4EBE6'\n",
    "colors = [GREEN, PURPLE]\n",
    "custom_params = {\n",
    "    'axes.spines.right': False, 'axes.spines.top': False,\n",
    "    'axes.facecolor':BACKGROUND, 'figure.facecolor': BACKGROUND, \n",
    "    'figure.figsize':(8, 8)\n",
    "}\n",
    "sns_palette = sns.color_palette(colors, len(colors))\n",
    "sns.set_theme(style='ticks', rc=custom_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a context manager to download data from the cloud.\n",
    "s3 = S3(s3root='s3://outerbounds-datasets/taxi')\n",
    "\n",
    "# Use Metaflow S3 client to get the latest file.\n",
    "# This file is being updated every hour, simulating the changing, drifting, and sometimes broken nature of production data streams.\n",
    "obj = s3.get('latest.parquet')\n",
    "# The goal is to write a flow that builds and cross-validates a model to predicts the total fare of each taxi rid (row) in the dataset.\n",
    "# Since the data is changing, Task 2 and 3 asks you to deploy a flow to production via Argo workflows, so your workflow can run automatically when this file changes in S3.\n",
    "\n",
    "# Load the contents of the parquet file in memory.\n",
    "df = pd.read_parquet(obj.path)\n",
    "\n",
    "s3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obviously_bad_data_filters = [\n",
    "\n",
    "    df.fare_amount > 0,         # fare_amount in US Dollars\n",
    "    df.trip_distance <= 100,    # trip_distance in miles\n",
    "    df.trip_distance > 0\n",
    "\n",
    "    # TODO: add some logic to filter out what you decide is bad data!\n",
    "    # TIP: Don't spend too much time on this step for this project though, it practice it is a never-ending process.\n",
    "\n",
    "]\n",
    "\n",
    "for f in obviously_bad_data_filters:\n",
    "    df = df[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1, figsize=(6,6))\n",
    "ax.scatter(df.trip_distance, df.fare_amount, color=GREEN, alpha=0.3)\n",
    "ax.set_xlabel(\"Trip Distance\")\n",
    "ax.set_ylabel(\"Fare\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Write a flow\n",
    "\n",
    "In this task, you will deploy a workflow to Argo Workflows. The following flow is mostly written for you, minus a few TODO items.\n",
    "\n",
    "Notice the use of the `@trigger` flow-level decorator.\n",
    "In this flow, this is telling Argo to listen for events named `s3`, and to trigger a run of the flow when the S3 event is emitted. \n",
    "The S3 event your sandbox is listening for are the updates to the `latest.parquet` file mentioned in the comments in [Task 1](#task-1-eda). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../flows/cloud/event_triggered_linear_regression.py\n",
    "from metaflow import FlowSpec, step, card, conda_base, current, Parameter, Flow, trigger\n",
    "from metaflow.cards import Markdown, Table, Image, Artifact\n",
    "\n",
    "URL = \"https://outerbounds-datasets.s3.us-west-2.amazonaws.com/taxi/latest.parquet\"\n",
    "DATETIME_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "@trigger(events=['s3'])\n",
    "@conda_base(libraries={'pandas': '1.4.2', 'pyarrow': '11.0.0', 'numpy': '1.21.2', 'scikit-learn': '1.1.2'})\n",
    "class TaxiFarePrediction(FlowSpec):\n",
    "\n",
    "    data_url = Parameter(\"data_url\", default=URL)\n",
    "\n",
    "    def transform_features(self, df):\n",
    "\n",
    "        # TODO: \n",
    "            # Try to complete tasks 2 and 3 with this function doing nothing like it currently is.\n",
    "            # Understand what is happening.\n",
    "            # Revisit task 1 and think about what might go in this function.\n",
    "            \n",
    "        return df\n",
    "\n",
    "    @step\n",
    "    def start(self):\n",
    "\n",
    "        import pandas as pd\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        self.df = self.transform_features(pd.read_parquet(self.data_url))\n",
    "\n",
    "        # NOTE: we are split into training and validation set in the validation step which uses cross_val_score.\n",
    "        # This is a simple/naive way to do this, and is meant to keep this example simple, to focus learning on deploying Metaflow flows.\n",
    "        # In practice, you want split time series data in more sophisticated ways and run backtests. \n",
    "        self.X = self.df[\"trip_distance\"].values.reshape(-1, 1)\n",
    "        self.y = self.df[\"total_amount\"].values\n",
    "        self.next(self.linear_model)\n",
    "\n",
    "    @step\n",
    "    def linear_model(self):\n",
    "        \"Fit a single variable, linear model to the data.\"\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "\n",
    "        # TODO: Play around with the model if you are feeling it.\n",
    "        self.model = LinearRegression()\n",
    "\n",
    "        self.next(self.validate)\n",
    "\n",
    "    def gather_sibling_flow_run_results(self):\n",
    "\n",
    "        # storage to populate and feed to a Table in a Metaflow card\n",
    "        rows = []\n",
    "\n",
    "        # loop through runs of this flow \n",
    "        for run in Flow(self.__class__.__name__):\n",
    "            if run.id != current.run_id:\n",
    "                if run.successful:\n",
    "                    icon = \"✅\" \n",
    "                    msg = \"OK\"\n",
    "                    score = str(run.data.scores.mean())\n",
    "                else:\n",
    "                    icon = \"❌\"\n",
    "                    msg = \"Error\"\n",
    "                    score = \"NA\"\n",
    "                    for step in run:\n",
    "                        for task in step:\n",
    "                            if not task.successful:\n",
    "                                msg = task.stderr\n",
    "                row = [Markdown(icon), Artifact(run.id), Artifact(run.created_at.strftime(DATETIME_FORMAT)), Artifact(score), Markdown(msg)]\n",
    "                rows.append(row)\n",
    "            else:\n",
    "                rows.append([Markdown(\"✅\"), Artifact(run.id), Artifact(run.created_at.strftime(DATETIME_FORMAT)), Artifact(str(self.scores.mean())), Markdown(\"This run...\")])\n",
    "        return rows\n",
    "                \n",
    "    \n",
    "    @card(type=\"corise\")\n",
    "    @step\n",
    "    def validate(self):\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        self.scores = cross_val_score(self.model, self.X, self.y, cv=5)\n",
    "        current.card.append(Markdown(\"# Taxi Fare Prediction Results\"))\n",
    "        current.card.append(Table(self.gather_sibling_flow_run_results(), headers=[\"Pass/fail\", \"Run ID\", \"Created At\", \"R^2 score\", \"Stderr\"]))\n",
    "        self.next(self.end)\n",
    "\n",
    "    @step\n",
    "    def end(self):\n",
    "        print(\"Success!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    TaxiFarePrediction()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ../flows/cloud/event_triggered_linear_regression.py --environment=conda run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Promote to Production"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the workflow in the previous section is running and logging results as you like, it is time to deploy this workflow to production, so it can run in the cloud without manually triggering it. \n",
    "\n",
    "You can read more in [Metaflow's Argo documentation](https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-argo-workflows)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the workflow to Argo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If you run from this from the terminal you may need to adjust the path to the flow file, depending on where you saved it - what comes after %%writefile? \n",
    "! python ../flows/cloud/event_triggered_linear_regression.py --environment=conda --with retry argo-workflows create "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, go to the Argo deployment attached to your sandbox by finding your ID from the URL in your browser.\n",
    "\n",
    "Your sandbox has a unique ID, visible in the URL with this pattern.\n",
    "```\n",
    "https://vs-<YOUR ID>.outerbounds.dev/<DELETE EVERYTHING AFTER THIS>\n",
    "```\n",
    "\n",
    "Remove the `vs` part in front, and everything after `.dev`, then navigate to your sandboxes Argo deployment at:\n",
    "```\n",
    "https://argo-<YOUR ID>.outerbounds.dev/\n",
    "```\n",
    "\n",
    "Navigating to the \"Workflow Templates\" section of the Argo UI, you should find your workflow has been deployed:\n",
    "\n",
    "<img src=\"../img/argo-workflow-template.png\" width=\"1200px\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually trigger the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ../flows/cloud/event_triggered_linear_regression.py --environment=conda argo-workflows trigger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After triggering the worfklow, return to your Argo dashboard and navigate to the `Workflows` section to see your flow executing:\n",
    "\n",
    "<img src=\"../img/argo-workflow.png\" width=\"1200px\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is the flow \"automatically\" triggered? \n",
    "\n",
    "In the previous section, you manually triggered the deployed flow using a Metaflow command. \n",
    "So how will it run without that manual trigger in production? \n",
    "\n",
    "1. There is a script running in the background in your sandbox that looks at the `latest.parquet` file in S3 and emits an `S3` when it has changed since the last `TaxiFarePrediction` flow run was created. As mentioned earlier, the file is being updated every hour, time shifting the [NYC taxi data stream from January 2023](https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet).\n",
    "2. When the events are emitted, the same thing happens as the command you ran in [the previous section](#manually-trigger-the-workflow)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Task 4: Handle Failures! \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you deployed the flow in the previous task, excellent job! \n",
    "Now it is time to harden the workflow so it is robust to the vagaries of real world data.\n",
    "\n",
    "Specifically, at some point we will intentionally corrupt some of the training data, to simulate a common real world occurence.\n",
    "\n",
    "Can you,\n",
    "1. Figure out what is happening in the new data that is causing an error?\n",
    "2. Add a few lines of code to your `TaxiFarePrediction` flow's `start` step that will make your flow robust to this kind of data quality issue?\n",
    "3. How would you refactor the `TaxiFarePrediction` to use the decorators you learned in this week's lesson, such as `@catch`, `@retry`, etc.? \n",
    "\n",
    "(Optional) TODO: Write a report of your findings to these questions.\n",
    "(Optional) TODO: Update your production deployment of `TaxiFarePrediction` with the new approach(es)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "full-stack-metaflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
